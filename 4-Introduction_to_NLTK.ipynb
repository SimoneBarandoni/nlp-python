{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"     The day for the race came, and    the Tortoise and Hare  started   together.        The Tortoise never stopped for a moment, walking slowly but steadily, right to the end of the course. The Hare ran fast and stopped to lie down for a rest. But he fell fast asleep. Eventually, he woke up and ran as fast as he could. But when he reached the end, he saw the Tortoise there already, sleeping comfortably after her effort.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower() is a Python function for strings\n",
    "lower_text = sample_text.lower()\n",
    "lower_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra spaces removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define a function to remove extra spaces. It uses the split() function to divide a string into words by deleting\n",
    "# spaces. Then, re-join the string together separating words with single spaces.\n",
    "\n",
    "def remove_whitespace(text):\n",
    "    return  \" \".join(text.split())\n",
    "\n",
    "lower_text = remove_whitespace(lower_text)\n",
    "lower_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the output is a list, where each element is a sentence of the original text\n",
    "nltk.sent_tokenize(lower_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization of the whole text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the output is a list, where each element is a token of the original text\n",
    "tokenized_text = nltk.word_tokenize(lower_text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we import the list of the english stopwords and save it into stopwords_en\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_en = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we prepare a empty list, which will contain the words after the stopwords removal\n",
    "tokenized_text_2 = []\n",
    "\n",
    "# we iterate into the list of tokens obtained through the tokenization\n",
    "for token in tokenized_text:\n",
    "    # if a token is not a stopword, we insert it in the list\n",
    "    if token not in stopwords_en:\n",
    "        tokenized_text_2.append(token)\n",
    "\n",
    "# the output is a list of all the tokens of the original text excluding the stopwords\n",
    "print(tokenized_text_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The output of the POS Tagging is a list of tuples. A tuple is a collection which is ordered and unchangeable. Note that you can understand that it is a tuple since it is like a list but between round brackets\n",
    "\n",
    "#### List = [ element 1, element 2, ... ]\n",
    "#### Tuple = (element 1, element 2, ...)\n",
    "#### Set = {element 1, element 2, ...}\n",
    "#### Dictionary = { key 1 : elements 1, key 2 : elements 2, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagging = nltk.pos_tag(tokenized_text_2)\n",
    "print(pos_tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation Removal\n",
    "\n",
    "Now we want to remove the punctuation. As you can see, punctuation does not have a specific POS Tag. All the other POS have a label, composed by at least two characters: \"NN\", \"VBD\", etc., while for punctuation the tag is the mark itself: \".\", \",\", etc.\n",
    "So, to remove punctuation, we can remove all the tokens whose POS tag has length = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_POS_text = []\n",
    "\n",
    "for tuple in pos_tagging:\n",
    "    # POS tagged text is a list of tuples, where the first element tuple[0] is a token and the second one tuple[1] is\n",
    "    # the Part of Speech. If the POS has length == 1, the token is punctuation, otherwise it is not, and we insert it\n",
    "    # in the list cleaned_POS_text\n",
    "    if len(tuple[1]) > 1:\n",
    "        cleaned_POS_text.append(tuple)\n",
    "        \n",
    "print(cleaned_POS_text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk.pos_tag returns many different Part of Speech tags. For the next step, lemmatization, it is better to simplify these tags. To do it we use the following function to substitute the POS with an easier form. E.g. from \"VBD\", \"VBG\", etc. (which are verbs) we write \"v\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpler_pos_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return \"a\"\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return \"v\"\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return \"n\"\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return \"r\"\n",
    "    else:         \n",
    "        return None\n",
    "    \n",
    "simpler_POS_text = []\n",
    "\n",
    "# for each tuple of the list, we create a new tuple: the first element is the token, the second is\n",
    "# the simplified pos tag, obtained calling the function simpler_pos_tag()\n",
    "# then we append the new created tuple to a new list, which will be the output\n",
    "for tuple in cleaned_POS_text:\n",
    "    POS_tuple = (tuple[0], simpler_pos_tag(tuple[1]))\n",
    "    simpler_POS_text.append(POS_tuple)\n",
    "    \n",
    "print(simpler_POS_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **lemmatize()** of WordNet, can be executed with or without giving it the POS as second argument.<br>\n",
    "1 - lemmatize(token)<br>\n",
    "2 - lemmatize(token, pos=\"...\")<br>\n",
    "If we give the POS, it will perform the lemmatization with a better accuracy <br>\n",
    "The POS tags that this function can read are the simplified ones, that we produced previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('cars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('was', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_text = []\n",
    "\n",
    "for tuple in simpler_POS_text:\n",
    "    if (tuple[1] == None):\n",
    "        lemmatized_text.append(lemmatizer.lemmatize(tuple[0]))\n",
    "    else:\n",
    "        lemmatized_text.append(lemmatizer.lemmatize(tuple[0], pos=tuple[1]))\n",
    "    \n",
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_text = []\n",
    "\n",
    "for tuple in simpler_POS_text:\n",
    "    stem_text.append(stemmer.stem(tuple[0]))\n",
    "        \n",
    "print(stem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem(\"I was feeding two dogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
